{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%run utils.ipynb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_df,fuzzy_df,attack_free_df=load_data(\"out_paths\",lib=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_dos_df=dos_df[dos_df[\"updated_flag\"]=='T']\n",
    "only_fuzzy_df=fuzzy_df[fuzzy_df[\"updated_flag\"]=='T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_free_inside_dos_df=dos_df[dos_df[\"updated_flag\"]=='R']\n",
    "attack_free_inside_fuzzy_df=fuzzy_df[fuzzy_df[\"updated_flag\"]=='R']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_column_in_dataframe(df, column_name):\n",
    "    \"\"\"\n",
    "    Checks column exist or not in given df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df :pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    column_name : str\n",
    "        Column name that will be checked.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "       If the specified column does not exist in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the attack-free dataset inside the fuzzy dataset, the value 6 in the dlc column appears only three times out of 3 million records. Since it is noisy, it needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>can_id</th>\n",
       "      <th>dlc</th>\n",
       "      <th>byte_0</th>\n",
       "      <th>byte_1</th>\n",
       "      <th>byte_2</th>\n",
       "      <th>byte_3</th>\n",
       "      <th>byte_4</th>\n",
       "      <th>byte_5</th>\n",
       "      <th>byte_6</th>\n",
       "      <th>byte_7</th>\n",
       "      <th>updated_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1546675</th>\n",
       "      <td>1.478197e+09</td>\n",
       "      <td>0105</td>\n",
       "      <td>6</td>\n",
       "      <td>eb</td>\n",
       "      <td>01</td>\n",
       "      <td>b7</td>\n",
       "      <td>00</td>\n",
       "      <td>98</td>\n",
       "      <td>02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713142</th>\n",
       "      <td>1.478197e+09</td>\n",
       "      <td>0105</td>\n",
       "      <td>6</td>\n",
       "      <td>ec</td>\n",
       "      <td>01</td>\n",
       "      <td>b8</td>\n",
       "      <td>00</td>\n",
       "      <td>be</td>\n",
       "      <td>01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713159</th>\n",
       "      <td>1.478197e+09</td>\n",
       "      <td>0105</td>\n",
       "      <td>6</td>\n",
       "      <td>eb</td>\n",
       "      <td>01</td>\n",
       "      <td>b7</td>\n",
       "      <td>00</td>\n",
       "      <td>98</td>\n",
       "      <td>02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp can_id  dlc byte_0 byte_1 byte_2 byte_3 byte_4 byte_5  \\\n",
       "1546675  1.478197e+09   0105    6     eb     01     b7     00     98     02   \n",
       "1713142  1.478197e+09   0105    6     ec     01     b8     00     be     01   \n",
       "1713159  1.478197e+09   0105    6     eb     01     b7     00     98     02   \n",
       "\n",
       "        byte_6 byte_7 updated_flag  \n",
       "1546675    NaN    NaN            R  \n",
       "1713142    NaN    NaN            R  \n",
       "1713159    NaN    NaN            R  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_free_inside_fuzzy_df[attack_free_inside_fuzzy_df[\"dlc\"]==6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_free_inside_fuzzy_df=attack_free_inside_fuzzy_df[attack_free_inside_fuzzy_df[\"dlc\"]!=6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_sampling(df, sample_size):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Perform random sampling on a given DataFrame.\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe from which to sample data.\n",
    "    sample_size : int\n",
    "        The number of samples to extract\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A randomly sampled DataFrame with 'sample_size' rows\n",
    "    \"\"\"\n",
    "    return df.sample(n=sample_size,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_proportionate_stratified_sampling(df,column_name, sample_fraction):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Perform proportionate stratified sampling on a given DataFrame.\n",
    "\n",
    "    This function samples a specified fraction of each unique category \n",
    "    in the given column, ensuring the original distribution is maintained.\n",
    "    ----------\n",
    "    df : _type_\n",
    "        The input DataFrame containing data.\n",
    "    column_name : _type_\n",
    "        The name of column to use for stratified sampling.\n",
    "    sample_fraction : _type_\n",
    "        The fraction of data to sample from each category. (between 0 and 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A proportionately stratified sample of the input DataFrame.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the sample_fraction is not between 0 and 1.\n",
    "    \"\"\" \n",
    "\n",
    "    if not (0<sample_fraction<=1):\n",
    "        raise ValueError(\"sample_fraction must be between 0 and 1\")\n",
    "    \n",
    "    #group by creates sub-dataframes for each unique value in the column\n",
    "    #apply allows us to apply a function to each of these sub-dataframes\n",
    "    #lambda applies sample to each sub-dataframe\n",
    "    \n",
    "    return df.groupby(column_name, group_keys=False).apply(lambda x: x.sample(frac=sample_fraction,random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dos_df=do_random_sampling(only_dos_df, 40000)\n",
    "sampled_fuzzy_df=do_random_sampling(only_fuzzy_df, 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naz\\AppData\\Local\\Temp\\ipykernel_3436\\2371874293.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(column_name, group_keys=False).apply(lambda x: x.sample(frac=sample_fraction,random_state=42))\n",
      "C:\\Users\\Naz\\AppData\\Local\\Temp\\ipykernel_3436\\2371874293.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(column_name, group_keys=False).apply(lambda x: x.sample(frac=sample_fraction,random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19778, 12)\n",
      "(9235, 12)\n",
      "(10041, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naz\\AppData\\Local\\Temp\\ipykernel_3436\\2371874293.py:34: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(column_name, group_keys=False).apply(lambda x: x.sample(frac=sample_fraction,random_state=42))\n"
     ]
    }
   ],
   "source": [
    "sampled_attack_free_df=do_proportionate_stratified_sampling(attack_free_df,\"dlc\",0.02)\n",
    "sampled_attack_free_inside_dos_df=do_proportionate_stratified_sampling(attack_free_inside_dos_df,\"dlc\",0.003)\n",
    "sampled_attack_free_inside_fuzzy_df=do_proportionate_stratified_sampling(attack_free_inside_fuzzy_df,\"dlc\",0.003)\n",
    "\n",
    "print(sampled_attack_free_df.shape)\n",
    "print(sampled_attack_free_inside_dos_df.shape)\n",
    "print(sampled_attack_free_inside_fuzzy_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_df_by_column(df,column_name):\n",
    "    \"\"\"\n",
    "\n",
    "    Sort the given DataFrame by the values in the specified column.\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame to sort.\n",
    "    column_name : str\n",
    "        The name of the column to use for sorting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The input DataFrame sorted by the values in the specified column.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the column name is not found in the DataFrame.\n",
    "    \"\"\"\n",
    "    validate_column_in_dataframe(df,column_name)\n",
    "    return df.sort_values(by=column_name,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_multiple_dfs_by_column(dfs,column_name):\n",
    "    \"\"\"\n",
    "    Sort multiple DataFrames by the values in the specified column.\n",
    "    ----------      \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list\n",
    "        List of DataFrames to sort.\n",
    "    column_name : str\n",
    "        The name of the column to use for sorting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of DataFrames sorted by the values in the specified column.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the column name is not found in any of the DataFrames.\n",
    "    \"\"\"\n",
    "    sorted_dfs=[]\n",
    "    for df in dfs:\n",
    "        validate_column_in_dataframe(df,column_name)\n",
    "        sorted_dfs.append(df.sort_values(by=column_name,ascending=True))\n",
    "    return sorted_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[sampled_dos_df,sampled_fuzzy_df,sampled_attack_free_df,sampled_attack_free_inside_dos_df,sampled_attack_free_inside_fuzzy_df]\n",
    "column_name=\"timestamp\"\n",
    "sorted_dfs= sort_multiple_dfs_by_column(df_list,column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dos_df,sorted_fuzzy_df,sorted_attack_free_df,sorted_attack_free_inside_dos_df,sorted_attack_free_inside_fuzzy_df=sorted_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updated_flag into attack free df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_column(df,new_column_name):\n",
    "    \"\"\"\n",
    "    Insert new column into a DataFrame, initializing with missing values(pd.NA)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df :pd.DataFrame\n",
    "        The input DataFrame that new colum will be added.\n",
    "    new_column_name : str\n",
    "        The name of the new column to add to the DataFrame.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    df[new_column_name]=pd.NA\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updated_flag -> flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(df,column_name,new_column_name):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : _type_\n",
    "        _description_\n",
    "    column_name : _type_\n",
    "        _description_\n",
    "    new_column_name : _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    validate_column_in_dataframe(df,column_name)\n",
    "    return df.rename(columns={column_name:new_column_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_multiple_dfs_columns(dfs,column_name,new_column_name):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : _type_\n",
    "        _description_\n",
    "    column_name : _type_\n",
    "        _description_\n",
    "    new_column_name : _type_\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _type_\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    renamed_dfs=[]\n",
    "    for df in dfs:\n",
    "        renamed_dfs.append(rename_column(df,column_name,new_column_name))\n",
    "    return renamed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp', 'can_id', 'dlc', 'byte_0', 'byte_1', 'byte_2', 'byte_3',\n",
       "       'byte_4', 'byte_5', 'byte_6', 'byte_7', 'updated_flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'updated_flag' not found in            timestamp can_id  frame_type  dlc byte_0 byte_1 byte_2 byte_3  \\\n32      1.479121e+09   0329           0    8     87     b9     7e     14   \n46      1.479121e+09   018f           0    8     fe     36     00     00   \n209     1.479121e+09   0130           0    8     06     80     00     ff   \n294     1.479121e+09   02c0           0    8     14     00     00     00   \n387     1.479121e+09   0545           0    8     d8     00     00     8a   \n...              ...    ...         ...  ...    ...    ...    ...    ...   \n988552  1.479122e+09   0130           0    8     8a     7f     00     ff   \n988644  1.479122e+09   0153           0    8     00     00     00     ff   \n988706  1.479122e+09   0131           0    8     e9     7f     00     00   \n988750  1.479122e+09   02c0           0    8     14     00     00     00   \n988868  1.479122e+09   018f           0    8     fe     31     00     00   \n\n       byte_4 byte_5 byte_6 byte_7  \n32         12     20     00     14  \n46         00     3c     00     00  \n209        0b     80     0a     44  \n294        00     00     00     00  \n387        00     00     00     00  \n...       ...    ...    ...    ...  \n988552     91     80     06     1c  \n988644     00     ff     00     00  \n988706     0b     7f     0e     92  \n988750     00     00     00     00  \n988868     00     4b     00     00  \n\n[19778 rows x 12 columns].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m renamed_dfs\u001b[38;5;241m=\u001b[39m\u001b[43mrename_multiple_dfs_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdated_flag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mrename_multiple_dfs_columns\u001b[1;34m(dfs, column_name, new_column_name)\u001b[0m\n\u001b[0;32m     18\u001b[0m renamed_dfs\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs:\n\u001b[1;32m---> 20\u001b[0m     renamed_dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m renamed_dfs\n",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m, in \u001b[0;36mrename_column\u001b[1;34m(df, column_name, new_column_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename_column\u001b[39m(df,column_name,new_column_name):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"_summary_\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m        _description_\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mvalidate_column_in_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{column_name:new_column_name})\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mvalidate_column_in_dataframe\u001b[1;34m(df, column_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mChecks column exist or not in given df.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m   If the specified column does not exist in the DataFrame.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Column 'updated_flag' not found in            timestamp can_id  frame_type  dlc byte_0 byte_1 byte_2 byte_3  \\\n32      1.479121e+09   0329           0    8     87     b9     7e     14   \n46      1.479121e+09   018f           0    8     fe     36     00     00   \n209     1.479121e+09   0130           0    8     06     80     00     ff   \n294     1.479121e+09   02c0           0    8     14     00     00     00   \n387     1.479121e+09   0545           0    8     d8     00     00     8a   \n...              ...    ...         ...  ...    ...    ...    ...    ...   \n988552  1.479122e+09   0130           0    8     8a     7f     00     ff   \n988644  1.479122e+09   0153           0    8     00     00     00     ff   \n988706  1.479122e+09   0131           0    8     e9     7f     00     00   \n988750  1.479122e+09   02c0           0    8     14     00     00     00   \n988868  1.479122e+09   018f           0    8     fe     31     00     00   \n\n       byte_4 byte_5 byte_6 byte_7  \n32         12     20     00     14  \n46         00     3c     00     00  \n209        0b     80     0a     44  \n294        00     00     00     00  \n387        00     00     00     00  \n...       ...    ...    ...    ...  \n988552     91     80     06     1c  \n988644     00     ff     00     00  \n988706     0b     7f     0e     92  \n988750     00     00     00     00  \n988868     00     4b     00     00  \n\n[19778 rows x 12 columns]."
     ]
    }
   ],
   "source": [
    "renamed_dfs=rename_multiple_dfs_columns(sorted_dfs,\"updated_flag\",\"flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### timestamp->datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We convert timestamp to datetime for data visualization as a next step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_timestamp_to_datetime(df,column_name,new_column_name):\n",
    "    #ten digit timestamp suggessts seconds since epoch\n",
    "    \"\"\"\n",
    "    Convert a Unix timestamp to a datetime object and add it as a new column to the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the timestamp column.\n",
    "    column_name : str\n",
    "        The name of the column containing the Unix timestamp.\n",
    "    new_column_name : str\n",
    "        The name of the new column to add to the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the new column added.\n",
    "    \"\"\"\n",
    " \n",
    "    validate_column_in_dataframe(df,column_name)\n",
    "    \n",
    "    df[new_column_name]=pd.to_datetime(df[column_name],unit='s')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multiple_dfs_timestamp_to_datetime(dfs,column_name,new_column_name):\n",
    "    \"\"\"\n",
    "    Convert a Unix timestamp to a datetime object and add it as a new column to each DataFrame in the list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list\n",
    "        List of DataFrames containing the timestamp column.\n",
    "    column_name : str\n",
    "        The name of the column containing the Unix timestamp.\n",
    "    new_column_name : str\n",
    "        The name of the new column to add to the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of DataFrames with the new column added.\n",
    "    \"\"\"\n",
    "    converted_dfs=[]\n",
    "    for df in dfs:\n",
    "        converted_dfs.append(convert_column_timestamp_to_datetime(df,column_name,new_column_name))\n",
    "    return converted_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_timestamp_dfs=convert_multiple_dfs_timestamp_to_datetime(renamed_dfs,\"timestamp\",\"datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_timestamp_dfs[1].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can_id -> hex(str) to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_hex_to_int(df,column_name,new_column_name):\n",
    "    \"\"\"\n",
    "    Convert a hexadecimal string to an integer and add it as a new column to the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing the hexadecimal column.\n",
    "    column_name : str\n",
    "        The name of the column containing the hexadecimal string.\n",
    "    new_column_name : str\n",
    "        The name of the new column to add to the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    validate_column_in_dataframe(df,column_name)\n",
    "    df[new_column_name]=df[column_name].apply(lambda x: int(x,16))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multiple_dfs_str_hex_to_int(dfs,column_name,new_column_name):\n",
    "    \"\"\"\n",
    "    Convert a hexadecimal string to an integer and add it as a new column to each DataFrame in the list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list\n",
    "        List of DataFrames containing the hexadecimal column.\n",
    "    column_name : str\n",
    "        The name of the column containing the hexadecimal string.\n",
    "    new_column_name : str\n",
    "        The name of the new column to add to the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of DataFrames with the new column added.\n",
    "    \"\"\"\n",
    "    converted_dfs=[]\n",
    "    for df in dfs:\n",
    "        converted_dfs.append(convert_str_hex_to_int(df,column_name,new_column_name))\n",
    "    return converted_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_can_id_dfs=convert_multiple_dfs_str_hex_to_int(converted_timestamp_dfs,\"can_id\",\"can_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_can_id_dfs[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fuzzy_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naz-ids-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
